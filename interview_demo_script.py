"""
OnePath Interview Demo Script - Complete Walkthrough
==================================================

This script demonstrates the complete LangChain-based agent system
for the OnePath.ai interview. Run this to show your technical mastery.
"""

import asyncio
import json
from langchain_agent_system import OnepathAgentOrchestrator, ServiceRequest, FollowUpRequest

class InterviewDemoRunner:
    """
    Complete interview demonstration runner
    
    Interview Strategy: Use this to show your system in action
    """
    
    def __init__(self):
        self.orchestrator = OnepathAgentOrchestrator()
        
    async def run_complete_demo(self):
        """
        Run complete interview demonstration
        
        This shows everything the interviewer wants to see
        """
        
        print("ğŸ¯ ONEPATH INTERVIEW DEMONSTRATION")
        print("=" * 50)
        print()
        print("Demonstrating:")
        print("âœ… LangChain agent orchestration")
        print("âœ… OpenAI function calling")  
        print("âœ… Multi-step reasoning (MCP-style)")
        print("âœ… Dynamic prompt composition")
        print("âœ… Error handling and fallbacks")
        print("âœ… Production-ready FastAPI integration")
        print()
        
        # Scenario 1: Initial AC Repair Request
        await self._demo_initial_request()
        
        print("\n" + "="*50)
        
        # Scenario 2: Bundle Followup Request
        await self._demo_bundle_followup()
        
        print("\n" + "="*50)
        
        # Scenario 3: Error Handling
        await self._demo_error_handling()
        
        print("\nğŸ† INTERVIEW DEMONSTRATION COMPLETE!")
        
    async def _demo_initial_request(self):
        """Demo: Initial AC repair request"""
        
        print("ğŸ“‹ SCENARIO 1: INITIAL SERVICE REQUEST")
        print("-" * 40)
        print()
        
        # Customer request
        request = ServiceRequest(
            message="My AC is broken. Can someone fix it this week?",
            customer_id="interview-demo-001",
            location="123 Main St, Austin TX",
            preferred_time="weekday afternoon"
        )
        
        print(f"Customer Says: '{request.message}'")
        print()
        
        # Interview Commentary
        print("ğŸ§  INTERVIEW COMMENTARY:")
        print("- Agent will first analyze the request using custom analysis tool")
        print("- Then check calendar availability using calendar tool")
        print("- Finally calculate pricing using pricing tool")  
        print("- This demonstrates LangChain tool orchestration")
        print()
        
        # Process request
        print("ğŸ¤– AGENT PROCESSING:")
        response = await self.orchestrator.process_service_request(request)
        
        # Display results
        print("\nğŸ“Š RESULTS:")
        print(f"Request ID: {response.request_id}")
        print(f"Tools Used: {response.result.get('tools_used', [])}")
        print(f"Confidence: {response.confidence}")
        
        if 'recommendation' in response.result:
            print(f"\nğŸ’¬ Agent Recommendation:")
            print(response.result['recommendation'])
        
        print(f"\nğŸ¯ Next Steps: {', '.join(response.next_steps)}")
        
        # Store for followup demo
        self.demo_request_id = response.request_id
        
    async def _demo_bundle_followup(self):
        """Demo: Bundle followup request"""
        
        print("ğŸ“‹ SCENARIO 2: BUNDLE FOLLOWUP REQUEST")  
        print("-" * 40)
        print()
        
        # Customer followup
        followup_message = "Can you add thermostat installation too and bundle it?"
        
        followup = FollowUpRequest(
            request_id=self.demo_request_id,
            message=followup_message
        )
        
        print(f"Customer Followup: '{followup_message}'")
        print()
        
        # Interview Commentary
        print("ğŸ§  INTERVIEW COMMENTARY:")
        print("- Agent maintains conversation context using LangChain memory")
        print("- Will recalculate pricing with bundle optimization")
        print("- Demonstrates multi-turn conversation handling")
        print("- Shows business logic for service bundling")
        print()
        
        # Process followup
        print("ğŸ¤– AGENT PROCESSING:")
        response = await self.orchestrator.handle_followup(followup)
        
        # Display results
        print("\nğŸ“Š FOLLOWUP RESULTS:")
        print(f"Context Maintained: âœ…")
        print(f"Tools Used: {len(response.actions_taken)} actions")
        
        if 'updated_pricing' in response.result:
            pricing = response.result['updated_pricing']
            print(f"\nğŸ’° Updated Pricing:")
            print(f"  Total: ${pricing['total']:.2f}")
            print(f"  Bundle Savings: ${pricing['savings']:.2f}")
        
        if 'recommendation' in response.result:
            print(f"\nğŸ’¬ Agent Response:")
            print(response.result['recommendation'])
    
    async def _demo_error_handling(self):
        """Demo: Error handling and fallbacks"""
        
        print("ğŸ“‹ SCENARIO 3: ERROR HANDLING & FALLBACKS")
        print("-" * 40)
        print()
        
        # Interview Commentary
        print("ğŸ§  INTERVIEW COMMENTARY:")
        print("- System gracefully handles missing OpenAI API key")
        print("- Falls back to mock responses for demonstration")
        print("- Maintains all business logic and tool orchestration")
        print("- Shows production-ready error handling")
        print()
        
        # Show system status
        print("ğŸ” SYSTEM STATUS:")
        print(f"  LangChain Agent: {'âœ… Active' if self.orchestrator.agent else 'âŒ Fallback Mode'}")
        print(f"  OpenAI API: {'âœ… Connected' if self.orchestrator.openai_api_key else 'âŒ Mock Mode'}")
        print(f"  Tools Available: {len(self.orchestrator.tools)}")
        print(f"  Active Sessions: {len(self.orchestrator.active_sessions)}")
        
        print("\nâœ… Error handling and fallback mechanisms working correctly!")

def show_technical_architecture():
    """
    Show the technical architecture to interviewer
    
    Interview Strategy: Use this to explain your system design
    """
    
    architecture = """
    ğŸ—ï¸ TECHNICAL ARCHITECTURE
    =========================
    
    ğŸ“¡ API Layer (FastAPI)
    â”œâ”€â”€ /api/v1/service-request (POST) - Main service endpoint
    â”œâ”€â”€ /api/v1/followup/{id} (POST) - Followup handling  
    â”œâ”€â”€ /api/v1/session/{id} (GET) - Session management
    â””â”€â”€ /api/v1/health (GET) - Health monitoring
    
    ğŸ¤– Agent Layer (LangChain)
    â”œâ”€â”€ OnepathAgentOrchestrator - Main orchestrator
    â”œâ”€â”€ ChatOpenAI - GPT-4 integration
    â”œâ”€â”€ Custom Tools - Business logic integration
    â””â”€â”€ ConversationMemory - Context management
    
    ğŸ”§ Tools Layer (Function Calling)
    â”œâ”€â”€ analyze_customer_request - Request analysis
    â”œâ”€â”€ check_calendar_availability - Scheduling
    â””â”€â”€ calculate_service_pricing - Pricing & bundles
    
    ğŸ“Š Business Logic Layer
    â”œâ”€â”€ CalendarService - Mock scheduling API
    â”œâ”€â”€ PricingService - Business pricing logic
    â””â”€â”€ Service Type Detection - AI-powered analysis
    
    ğŸ›¡ï¸ Infrastructure Layer
    â”œâ”€â”€ Error Handling - Graceful degradation
    â”œâ”€â”€ Fallback Mechanisms - Mock mode support
    â”œâ”€â”€ Observability - Custom callbacks
    â””â”€â”€ Session Management - State persistence
    
    KEY DESIGN PRINCIPLES:
    âœ… Modular, microservice-ready architecture
    âœ… Production-grade error handling
    âœ… LangChain best practices implementation
    âœ… OpenAI function calling optimization
    âœ… Business logic separation
    âœ… Scalable conversation management
    """
    
    print(architecture)

def show_interview_talking_points():
    """
    Show key talking points for the interview
    
    Interview Strategy: These are your key discussion points
    """
    
    talking_points = """
    ğŸ¯ KEY INTERVIEW TALKING POINTS
    ===============================
    
    1. ğŸ§  REACT ARCHITECTURE IMPLEMENTATION
    "I've implemented ReACT using LangChain's create_openai_functions_agent, 
     which provides the Thoughtâ†’Actionâ†’Observation loop. The agent reasons 
     about customer requests, selects appropriate tools, and observes results."
    
    2. ğŸ”§ FUNCTION CALLING & TOOL ORCHESTRATION  
    "I created custom LangChain tools that integrate with our business logic.
     The agent intelligently selects tools based on customer intent - analysis
     first, then calendar or pricing as needed."
    
    3. ğŸ“ PROMPT ENGINEERING & COMPOSITION
    "The system uses ChatPromptTemplate with dynamic context injection.
     Prompts adapt based on conversation history and customer needs, using
     MessagesPlaceholder for memory integration."
    
    4. ğŸ”„ MULTI-STEP REASONING (MCP-style)
    "The agent performs multi-call protocols - analyzing requests, checking
     availability, calculating pricing, all in logical sequence. Each step
     informs the next, just like MCP server hierarchies."
    
    5. ğŸ’¼ BUSINESS LOGIC INTEGRATION
    "I separated business logic into service classes (CalendarService, 
     PricingService) that simulate real API integrations. This shows how
     to integrate LangChain with existing business systems."
    
    6. ğŸ›¡ï¸ PRODUCTION-READY ERROR HANDLING
    "The system gracefully degrades - if OpenAI API fails, it falls back
     to mock responses while maintaining all business logic. This ensures
     reliability in production environments."
    
    7. ğŸ’¬ CONVERSATION CONTINUITY
    "Using ConversationBufferWindowMemory, the agent maintains context
     across multiple turns. This handles bundle requests and modifications
     naturally, like a human customer service agent."
    
    8. ğŸš€ SCALABLE FASTAPI ARCHITECTURE
    "Built as production-ready microservice with proper error handling,
     CORS configuration, and API documentation. Ready for containerization
     and deployment to platforms like Render or Fly.io."
    
    QUESTIONS TO ASK INTERVIEWER:
    â€¢ "What's OnePath's current agent architecture? How does this compare?"
    â€¢ "Are there specific external APIs I should integrate with?"
    â€¢ "What's the expected scale - requests per minute?"
    â€¢ "How do you currently handle conversation context?"
    â€¢ "What's your deployment stack - containers, Kubernetes?"
    """
    
    print(talking_points)

async def main():
    """Main demo runner"""
    
    print("ğŸ¯ OnePath Interview Demo - Choose Option:")
    print("1. Complete system demonstration")
    print("2. Technical architecture overview") 
    print("3. Interview talking points")
    print("4. All of the above")
    print()
    
    choice = input("Enter choice (1-4): ").strip()
    
    if choice == "1":
        demo = InterviewDemoRunner()
        await demo.run_complete_demo()
    elif choice == "2":
        show_technical_architecture()
    elif choice == "3":
        show_interview_talking_points() 
    elif choice == "4":
        show_technical_architecture()
        print("\n" + "="*50 + "\n")
        show_interview_talking_points()
        print("\n" + "="*50 + "\n")
        demo = InterviewDemoRunner()
        await demo.run_complete_demo()
    else:
        print("Invalid choice. Running complete demo.")
        demo = InterviewDemoRunner()
        await demo.run_complete_demo()

if __name__ == "__main__":
    asyncio.run(main())